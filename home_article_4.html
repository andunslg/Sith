<!DOCTYPE html>
<!--
*************************************
Template designed by Austin Gregory of
AwfulMedia.com. Before using or modifying this file
please read the included license
*************************************
Date: 6/7/2012
Author: Austin Gregory
Website: AwfulMedia.com
*************************************
-->

<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Sith - Next Generation Perception Analysis</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="css/bootstrap.css" rel="stylesheet">
    <style type="text/css">

    </style>
    <link href="css/bootstrap-responsive.css" rel="stylesheet">
    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le fav and touch icons -->
    <link rel="shortcut icon" href="ico/favicon.ico">
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="ico/apple-touch-icon-114-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="ico/apple-touch-icon-72-precomposed.png">
    <link rel="apple-touch-icon-precomposed" href="ico/apple-touch-icon-57-precomposed.png">
	<link href='http://fonts.googleapis.com/css?family=Lato:300' rel='stylesheet' type='text/css'>
  </head>

  <body>

    <div class="navbar navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container">
          <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="index.html"><span class="color-highlight">Sith</span> - Next Generation Perception Analysis</a>
          <div class="nav-collapse">
            <ul class="nav pull-right">
              <li><a href="index.html">Home</a></li>
              <li><a href="solutions.html">Solutions</a></li>
              <li><a href="documentation.html">Documentation</a></li>
              <li><a href="downloads.html">Downloads</a></li>
			  <li><a href="contributors.html">Contributors</a></li>
			<!--  <li class="active"><a href="about.html">About</a></li>  -->
            </ul>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>
	<div class="container">
	<hr>
      <div class="row">
        <div class="span12">
			
		  <div class="well">
			<h1>BIG DATA ANALYTICS</h1>
            <br>
            <h3>Big Data</h3>
            <p align="justify">&nbsp;</p>
            <p align="justify">Big data refers to huge collection of various complex data sets whose  size is beyond the ability of traditional data processing methods and typical  data base software to capture, store, manage, process, and analyze. The term  &lsquo;Big Data&rsquo; implies the exponential growth, availability and the use of  information gathered from  data sources  like Web server logs and Internet click stream data, social media activity  reports, log files that are constantly appending transactions, mobile-phone  call detail records, digital pictures and videos, information capturing sensors  etc. In practice, the term &ldquo;big data&rdquo; is used to refer to high volume, high  velocity, high variety and high complexity. The three Vs, the volume, variety  and velocity define the basic attributes of big data [13]. </p>
            <h4>Data Volume  as an attribute</h4>
         <br>
            <p align="justify">Data volume is the primary attribute of big  data. Textual data streaming in from social media networks, increasing amounts  of sensor data collected, transaction based data which are being collected in  massive amounts etc. contribute in spanning the volume of data.</p>
            <h4>Data type  variety as an attribute</h4>
            <br>
            <p  align="justify">Big data come from a great  variety of sources like Web sources, including logs and click streams, and  social media etc. The structured, semi structured and unstructured data  generating from those sources appear in massive number of different formats,  types and forms like text, sensor data, audio, video, click stream, log files,  etc.
            <h4>Data feed  Velocity as an attribute</h4>
<br>
<p  align="justify">The frequency of data  generation and the frequency of data delivery are extremely high in big data.  Big data, which have been produced in a very high rate, are collected in real  time as click stream data from web sites, sensor data etc and data processing  for many purposes also has to be done in a very high rate, in real time in  order to react quickly enough to meet the demand.</p>
            <h3>Big Data Analytics</h3>
            <br>
            <p align="justify">Big data analytics is the process of applying advanced analytic  techniques in the purposes of examining huge sets of data of variety of types,  in variety of size and formats, to uncover hidden patterns, unknown  correlations and other useful information.    <br>
              Big data analytics is a complex and a different process from traditional  data analytic rules, methods and tools. A special analytical process has to be  followed in big data analytics. Data acquisition and recording, information  extraction and cleaning, data integration, aggregation and representation,  Query processing, data modeling and analysis, and finally interpretation are  the steps in analyzing big data [1] and they are illustrated below.</p>
            <h4>Data  Acquisition and Recording</h4>
           <br>
            <p  align="justify">Recoding data from data generating sources  without discarding useful information is the very first step in analysis  process. There are some concerns on this. Some data are correlated, some has to  be processed online and we have to concern about data reduction by  intelligently processing raw data to a size that can be managed without any  loss. What is the data, how it has to be recorded and measured are another main  concern in data recording and acquisition. </p>
            <h4>Information  Extraction and Cleaning</h4>
            <br>
            <p  align="justify">The information recorded may be not in the  format which is ready to be analyzed. Thus it is required an information  extracting process that pulls out only the required information only from  underlying resources and formatting them as suitable for analysis purposes.  Recognizing valid data, removing incorrect data in related to data cleaning and  it is essential since fake or incorrect data may lead to false, incorrect or  incomplete analysis decisions.</p>
            <h4>Data  Integration, Aggregation, and Representation</h4>
            <br>
            <p  align="justify">Due to the heterogeneity of big data, it is not  enough of locating, identifying and citing the data. Since it is more complex,  it requires the use of differences in data structures and semantics which are  computer understandable.</p>
            <h4>Query  Processing, Data Modeling, and Analysis</h4>
          <br>
            <p  align="justify">Since Big Data is heterogeneous, untrustworthy,  inter-related and dynamic, big data mining and querying methods are different  from traditional methods. Big Data mining requires integrated, cleaned,  trustworthy and efficiently accessible data, scalable mining algorithms,  declaratively querying interfaces. </p>
            <h4>Interpretation</h4>
  <br>
            <p  align="justify">With the results of the analysis, the decision maker has to interpret  the final result in user understandable way. It is important to convey the  queried results using a system with a rich palette of visualization. <br>
            </p>
            <p  align="justify">Big data are growing rapidly in the modern era. For example, the number  of connected devices exceeded the world population before the end of 2011, and  will reach 15 billion in 2015 [2].  Thus  most of the effective systems platforms in current world which have to tackle  with big data should be consisted with five key features. <br>
              First, it must detect relevant patterns and correlations of big data and  the torrents of events so that the predictions in the business world, trend and  opportunities in sales and marketing, instance of risks in enterprises can be  interpreted. Second, such system platforms must be capable of powerful  analytics so that anyone can interpret and immediately understand what is  happening at the moment of truth. As the third element, the platforms should  have automation to initiate corrective actions based on recognized patterns.  Also those platforms have to be flexible enough to compose and assemble new  lightweight applications on the fly. Fourth, the platform should scales  elastically across cloud environments etc. As the fifth, the platform should  provide natural environment for collaboration. [2] <br>
              Big data was a problem but now an opportunity generator in business  world. There are so many advantages of big data analytics. Social-influencer  marketing, customer-base segmentation, recognition of sales and market  opportunities, understanding of business change, better planning and fore  casting, market sentiment trending, cost reduction, fraud detection, risk  management are some of them. Thus big data analytics plays a significant role  in modern world [13].  For example, huge  sets of data from Social Medias like facebook, tweets, blogs, review sites,  news articles and various web contents etc are processed in order to analyze  sentiments of the people and the analytical results can be used on many  purposes.<br>
              There are major challenges appear in Big Data analytics due to its three  Vs which are referred for high data volume, high data variety and high data  feed velocity. The biggest challenge faced by big data analytical platforms is  managing scale. In the other words, managing large and rapidly increasing  volume of data. Due to the limitations in storage, CPU processing power and the  speed of processing this has become a huge problem in Big Data world. Since the  data sets are heterogeneous and incomplete some other aspect of big data  analytic is being challenged. Timeliness is another challenge because  processing a given set of big data faster is less feasible in current data  analytical systems, in situations which the results of analysis are required  immediately. Thus designing big data analyzing platforms become particularly  challenging when the data volume is growing rapidly and the queries have tight  response time limits.  Maintaining  security and privacy is another challenge addressed by big data analytics. <br>
            Big data analytics has become more challenging due to the incapability  of the traditional data base management tools, data processing tools and  storage mechanisms etc. in big data managing. But with the revolution of big  data analytics, new methodologies, tools, concepts, programming models and  applications for managing, capturing, processing and interpreting big data.</p>
<h3>Existing programming and storage models for big data  analytics</h3>
            <h4>&nbsp;</h4>
            <h4>NoSQL Data  Bases</h4>
<br>
            <p  align="justify">NoSQL is interpreted as &lsquo;Not Only SQL&rsquo;, since  it came up with alternatives for Relational Data Base Management Systems  (RDBMS)/ SQL. NoSQL data bases are for the data storage of big data, when the  data does not require any relation model.</p>
            <h4>Motives for  the NoSQL Data Bases instead of RDBMS</h4>
      <br>
            <p  align="justify">The data management requirements of modern big  data related applications could not be satisfied with the use of RDBMSs, which  have been used as the default solution for many years so that it became  essential to move into a new approach of data management which can avoid the  drawbacks emerged from existing RDBMSs. Thus, NoSQL, was introduced as the  alternative approach, and the motives for NoSQL, instead of RDBMS are  illustrated in the list below.</p>
            <ul>
              <li>Throughput  of RDBMSs is too low and since that RDBMS was not capable of handling the  underlying data stores in processing large volumes of data sets (big data).  Some web applications required efficient data storage mechanism other than  RDBMS.</li>
              <li>RDBMS are  not horizontally scalable. Horizontal scaling means, adding more machines  (nodes) to a cluster, but vertical scaling means adding more hardware onto  existing machines. RDBMSs are vertically scalable, so that RDBMSs have  limitations because there is an upper bound on how much hardware that can be  added and utilized by software.  But  horizontal scaling does not have those limitations and it created a motivation  factor to move into NoSQL data stores, which supports horizontal scaling.</li>
              <li>Object  Oriented Programming (OOP) is the most prevalent programming paradigm in the  current world. If we consider an application objects, it requires a mapping  between an application&rsquo;s object model and the data base&rsquo;s relational model.  This process of mapping is too expensive since it takes time and effort. </li>
              <li>RDMBS can  be considered as a general too that can handle different applications with  different requirements of data management. This concept of &lsquo;One Size Fits All&rsquo;  was flawed because a general tool cannot satisfy all the requirements of  different applications. Because different applications which have different  requirements of performance, consistency, reliability, availability required  specific data management tools for each requirement [3].</li>
            </ul>
            <h4>Types of  NoSQL Databases</h4>
            <br>
            <p  align="justify">There  are four types of NoSQL databases used in distinctive approaches [4].</p>
            <ul>
              <li><strong>Key-value  stores</strong></li>
            </ul>
            <p  align="justify">Key- value stores  uses keys and their corresponding values, to store data in scheme-less way.  This enables the querying of millions of values in a very short time period  than time required in conventional data stores. Oracle NoSQL database is an  example for that.</p>
            <ul>
              <li><strong>Tabular  stores</strong></li>
            </ul>
            <p  align="justify">Tabular stores,  also known as Bigtable stores consist of tables which can have different schema  for each row and each row can be considered of having one huge extensible  column containing data. Widely used example of this type is BigTable and Apache  HBase[7].</p>
            <ul>
              <li><strong>Document  data bases</strong></li>
            </ul>
            <p  align="justify">This type of data  bases consist of a set of documents each of which contains fields of data in  standard formats like XML or Json. Data in this document stores, can be  structured in scheme less way as such collections. MonoDB, OrientDB are  examples for this type.</p>
            <ul>
              <li><strong>Graph  data bases</strong></li>
            </ul>
            <p  align="justify">Graph data bases have  a set of nodes linked together by edges.   Each node in the graph contains fields of data and the querying takes  place using efficient mathematical graph-traversal algorithms to achieve  performance. Neo4J and the graph layer of OrientDB are the examples for them.</p>
            <h4>Characteristics  of NoSQL Databases</h4>
<br>
            <p  align="justify">NoSQL data bases can be differentiated from  other data storage systems, because of the special characteristics of them.  Those are described below.</p>
            <ul>
              <li>NoSQL data  stores are built to store and process large volumes of data quickly.</li>
              <li>Data is  replicated over several machines of the cluster (distributed system) for the  purpose of redundancy of data to achieve high availability, failure recovery  and consistency.</li>
              <li>Horizontally  scalable or in the other words they are capable of dynamically adapting to the  additional of new servers or removing of existing servers within the cluster  without any downtime. There is no any upper bound for the addition of the  number of nodes [4].</li>
              <li>Follow BASE  approach, instead of ACID (Atomicity, Consistency, Isolation, and Durability).  Base stands for Basically Available, Soft-state, Eventual Consistency, which  means that the application is available all the time, but not always consistent  and will eventually in some state [3].</li>
              <li>The  structure of the data does not follow definitions of schemas and instead of  that the clients can store data as they desire, without being adhere to any  schema definition.</li>
              <li>Use non  relational data models, which are more complex and less rigid than relational  data models.</li>
              <li>SQL is  unsupported [5].</li>
              <li>Simple  interfaces are used for querying the data.</li>
              <li>Use of  distributed indexes to store key data values for efficient querying [4].</li>
            </ul>
            <h4>Apache  Cassandra</h4>
            <br>
            <p  align="justify">Apache Cassandra is an  open source distributed database management system to handle big data, spread  across many commodity servers. It is a NoSQL based solution which provides high  availability and scalability. Cassandra supports for replication across  multiple nodes in a cluster to achieve fault tolerance, and provides low  latency. Since Cassandra is decentralized, there are no single points of  failure, no network bottlenecks. Apart from that, it supports horizontal  scalability, with no downtime or interruption to applications.<br>
              Netflix, eBay, Twitter,  Urban Airship, Constant Contact, Cisco, Reddit, OpenX are some of the examples  where the Cassandra has been using as the database management system [5].</p>
            <h4>Map Reduce</h4>
            <br>
            <p  align="justify">Map reduce is a  programming model for processing and generating massive data sets. It can be  used to process parallelizable problems across huge data sets using cluster of  nodes (if all nodes are on the same local network) or a grid (if the nodes are  on a shared distributive systems). The computational processing of data can  occur for structured and unstructured data, so that the data stored in file  systems as unstructured data and the data stored in database systems as  structured data can be efficiently processed via Map Reduce programming  paradigm. <br>
              Map reduce allows for  distributed processing of the map and reduction operations. It uses a specific  map function that processes &lt;Key, Value&gt; pair to generate a set of  intermediate &lt;Key, Value&gt; pairs and a reduce function to merge all the  intermediate pairs associated with the same intermediate key [6].<br>
              In Map Reduce model,  there are three steps of processing is applied to a particular data set Those  phases are mapping, shuffling, and reducing the data records in order to  process them [8].</p>
            <ul>
              <li><strong>Map</strong></li>
            </ul>
            <p  align="justify">In this phase, a  Map or a User Defined Function (UDF) is executed on each record of the data  set. The file or the data set typically striped across many computers or nodes  and many processes or the Mappers work on the file in parallel [8]. As shown in  the Figure 1, the master node of the grid or the cluster of nodes takes the  input, divides it into smaller sub problems and distribute them in the worker  nodes. Each worker node follows the same procedure leading a multi level tree  structure and the worker node which processes the smaller problem, passes the  answer back to the master node. The output of each call to Map is a list of key  and value pairs. </p>
            <ul>
              <li><strong>Shuffle</strong></li>
            </ul>
            <p  align="justify">In this phase,  all the &lt;Key,Value&gt; pairs are sent to another set of nodes (computers)  such that all the pairs with the same Key go to the same node. At each  destination node, &lt;Key,Value&gt; pairs with same Key are aggregated  together. If &lt;x, y1&gt;, &lt;x, y2&gt;, &lt;x, y3&gt;…..&lt;x, yn&gt; are  the all &lt;Key, Value&gt; pairs generated by Mappers with same Key of &lsquo;x&rsquo;, at  the destination node for Key &lsquo;x&rsquo;, so that these pairs are aggregated into large  &lt;Key, Value&gt; pair such that &lt;x,{y1, y2, y3,….. yn}&gt; [8]. The  aggregated pair is called as Reduce Record, and its key is referred to as the  Reduce Key.</p>
            <ul>
              <li><strong>Reduce</strong></li>
            </ul>
            <p  align="justify">In Reduce phase,  a UDF, also called Reduce is applied to each Reduce Record through parallel  processes. Each process is called as Reducer. Ultimately, for each invocation  of Reduce, a few sets of records get written into local output file.</p>
            <p align="center"><img src="home_article_4_clip_image002.gif" alt="" width="486" height="241"><br>
              Figure 1 - Architecture of Map  Reduce <br></p>
             <p align="justify"> Map Reduce  automatically parallelizes and executes the program on large clusters of  machines while partitioning the input data, scheduling the program&rsquo;s execution  across set of machines, handling machine failures, and managing the  inter-machine communication in run time. A typical Map Reduce computation can  process many terabytes of data on hundreds or thousands of machines [6]. </p>
            <ul>
              <li><strong>Importance  of Map Reduce</strong></li>
            </ul>
            <p  align="justify">Compared to parallel  databases, Map Reduce has many advantages.   When a parallel DBMS is used to perform big data analytics, the inputs  has to be loaded to database and this loading phase is inconvenient and  unacceptably slow.  But Map Reduce is  storage system independent and it can process data without initially requiring  it to be loaded into database. Generally it is possible to run more than 50 map  reduce analysis over the data and complete analysis, even before it is possible  to load data into data base and complete a single analysis [6]. Apart from  that, complicated transformations can be easily expressed in Map Reduce than in  SQL.  <br>
              Map Reduce uses a pull  model for moving data between mappers and reducers which opposed to a push  model where mappers write directly to reducers and since that Map Reduce  implementations have fault tolerant properties. Literature provides evidences  that a dozen distinct data sets at Google more than 1PB in size and dozens more  hundreds of TBs in size that are processed daily using Map Reduce [6]. Thus Map  Reduce model and its implementations are trusted in fault tolerance in  processing of big data. <br>
              Map Reduce provides a  simple model for analyzing data in heterogeneous systems such as production  environments which contain a mix of storage systems like relational databases,  file systems etc. End users of Map Reduce can extend it to support new storage  systems like files stored in distributed systems, data base query results, data  stored in Bigtable, and structured input files such as B-trees using an  implementation of a simple reader and writer functions.<strong></strong><br>
              Increasing performance  is another concern in the implementation of Map reduce model. It does not need  a full scan of over the data and it avoids unnecessary data readings. <br>
              But there are some  issues with Map Reduce model. It cannot use indices and it implies a full scan  of all input data. That is inefficient. The inputs and outputs of Map reduce  are always simple files in a file system. In Map Reduce, it is required the use  of inefficient textual data formats. Those are the issues with Map Reduce data  processing [6].</p>
       <br>
            <h4>Apache Hadoop</h4>
            <br>
            <p  align="justify">Hadoop is the main open source implementation of Map Reduce  programming paradigm [14]. Hadoop is for distributed processing of extremely  large amounts of data (Data in petrabytes). In addition of the size, Hadoop can  handle any type of data such as pictures, log files, video, audio files, and  many more without concerning whether the data is structured or unstructured.  Since Hadoop can be used to process a wide array of data, it allows more closer  and accurate query representation. Data sets stored in Hadoop Distributed File  System are organized across cluster of computers and it is able to handle  anywhere from a single server to thousands of machines [9]. Hadoop provides  reliability by creating redundant data and with Hadoop it is not required to  build a schema prior to storing information in the file system. Scalability,  reliability and portability are the advantages of using Hadoop in Big data  analytics. <br>
              The structure of Hadoop is contained with cluster of  computes. The servers are grouped into several clusters to achieve efficiency.  There is a node in the network that gathers location information. Another node  is dedicated for data storage. Depending on the location information, the  Distributed File Syetem replicates data from one server node and stores it in  another node to achieve redundancy of data to avoid the risk of server failure.</p>
            <ul>
              <li><strong>HBase</strong></li>
            </ul>
            <p  align="justify">Apache HBase[7] is the data base used by Hadoop which  is a distributed and scalable data storage for structured big data. HBase is  appropriate for real-time random read/write access to the big data which are  being stored as column oriented store model. </p>
            <ul>
              <li><strong>Apache Hive</strong></li>
            </ul>
            <p  align="justify">Apache Hive is a data warehouse system for Hadoop  framework [10]. It provides the facility of summarization, ad-hoc queries, and  analysis of big data stored in Hadoop compatible file systems. Hive uses a SQL  like language called, HiveQL to query data and.</p>
            <ul>
              <li><strong>Why Hadoop in Facebook?</strong></li>
            </ul>
            <p  align="justify">Facebook is a very popular social network which has  been used by millions of users all around the world and it is one of the  prominent users of Apache Hadoop framework. Thus, such social network have some  specific and major requirements related with data processing, data storage etc.  The data management requirements of a social media network like facebook and  the reasons for using Hadoop for Facebook, instead of other data processing  tool are illustrated below. <br>
              Elasticity is a major requirement of storage in social media  networks. It should be able to add incremental capacity to storage with minimal  overhead with no downtime. In a case of handling rapid growth of the number of  users of a social media like facebook, it is required to add more capacity  rapidly and system should be capable of automatically balancing the load. High  write throughput is another significant feature of a storage system of social  network, since it stores tremendous amount of data. To achieve high  availability and disaster recovery in Facebook, it is required to provide a  service with high uptime to users that cover both planned and unplanned events.  Apart from that it should be able to manage the loss of data center with  minimum damage and minimum data loss and be able to serve data out of another  data center in minimum time period. Efficient, low-latency, strong consistency  of data, fault isolation, ability of retrieving data set in a particular range  are some other requirements [11]. Those are the requirements of Facebook  achieved by using Hadoop and HBase as the data processing tool and data store.  s or power outages.</p>
            <h3>Complex Event Processing</h3>
           <br>
            <p  align="justify">With the increase of popularity of online product reviewing sites,  online rating systems, and social media networks like Twitter, Facebook etc,  innumerable amount of events are being generated showing an exponential growth  in event generating rate. By capturing these user or application generated  events, processing and analyzing them lead to achieve magnificent goals in many  areas. But due to the complexity and high rate of generating events in massive  amounts, continuous event capturing in real time, smooth processing and  analyzing has become rally challenging in current world. Thus to overcome from  those existing challenges, the concept of Complex Event Processing has been introduced.<br>
              Complex event processing is the technology of real time processing and  analyzing complex and massive amounts of event streams which are constantly  generated by real world applications and operations [12]. CEP can be used to  identify meaningful patterns and relationships among unrelated streaming  events. CEP can be utilized to perform efficient event processing for two  categories of event based application. The applications which monitor  surveillance, manufacturing and financial trading requests belong to one  category which require low latency. The other category is the applications  based on web analytics and data ware housing which need to handle higher data  rates.<br>
              A CEP engine provides the runtime to perform Complex Event Processing  where it accept the user defined queries, match those accepted queries against  continuously streaming events and trigger an event when the relevant conditions  are satisfied.</p>
          <br>
            <h4>CEP Infrastructure</h4>
            <p align="center"><img src="home_article_4_clip_image004.jpg" alt="" width="602" height="284"><br>
            Figure 2 - CEP Infrastructure<br></p>
            <p align="justify">The CEP infrastructure  is consisted with a set of event sources, a set of event processing agents  (EPA), and a set of event sinks [105]. In figure 2, it is displayed that how  those main three components are integrated in CEP infrastructure. EPAs are  referred to continuous queries which are responsible of performing the tasks  like filtering, aggregating and correlating events or searching for a  particular event pattern. EPAs are connected with Event sources and Event Sinks  and sometimes EPAs can be interconnected. Each event source in CEP application  has to be registered in the system first and then EPAs are dined on registered  event source. Ultimately event sinks are registered and connected to EPAs to  access queried results. Most of the CEP systems are shows a horizontal behavior  since that the addition and removing of event sources, EPAs and event sinks are  supported by the infrastructure.</p>
            <h3><br>
            </h3>

<h3>References</h3>
<br/>
<ol>
  <li>DivyakantAgrawal,  Philip Bernstein, Elisa Bertino, Susan Davidson, UmeshwarDayal, Michael  Franklin, Johannes Gehrke, Laura Haas,, Alon Halevy, Jiawei Han, H. V.  Jagadish, AlexandrosLabrinidis, Sam Madden, YannisPapakonstantinou, Jignesh M.  Patel, Raghu Ramakrishnan, Kenneth Ross, Cyrus Shahabi, and Dan Suciu,  &ldquo;Challenges and Opportunities with Big Data.&rdquo; 2011.</li>
  <li>M.  O&rsquo;Connell, &ldquo;Big Data Analytics: Scaling Up and Out in the Event-Enabled  Enterprise,&rdquo; Wall Street Technology Association, Ticker, no. 3, 2012.</li>
  <li>P.  Näsholm, &ldquo;Extracting Data from NoSQL Databases-A Step towards Interactive             Visual Analysis of NoSQL Data,&rdquo;  2012. </li>
  <li>K.  Barmpis and D. S. Kolovos, &ldquo;Comparative Analysis of Data Persistence  Technologies for Large-Scale Models,&rdquo; 2012. </li>
  <li>&ldquo;The  Apache Cassandra Project.&rdquo; [Online]. Available: http://cassandra.apache.org/.  [Accessed: 29-Apr-2013].</li>
  <li>J.  Dean and S. Ghemawat, &ldquo;MapReduce: a flexible data processing tool,&rdquo;  Communications of the ACM, vol. 53, no. 1, pp. 72–77, 2010.</li>
  <li>&ldquo;HBase  - Apache HBase™  Home.&rdquo; [Online].  Available: http://hbase.apache.org/. [Accessed: 29-Apr-2013].</li>
  <li>Goel  and K. Munagala, &ldquo;Complexity Measures for Map-Reduce, and Comparison to  Parallel Computing,&rdquo; arXiv:1211.6526, Nov. 2012.</li>
  <li>D.  Becerra, T. Pham, and T. Williams, &ldquo;How Big Data is Revolutionizing Business,&rdquo;  2012.</li>
  <li>&ldquo;Welcome  to Hive!&rdquo; [Online]. Available: http://hive.apache.org/. [Accessed:  29-Apr-2013].</li>
  <li>D.  Borthakur, J. Gray, J. S. Sarma, K. Muthukkaruppan, N. Spiegelberg, H. Kuang,  K. Ranganathan, D. Molkov, A. Menon, and S. Rash, &ldquo;Apache Hadoop goes realtime  at Facebook,&rdquo; in Proceedings of the 2011 international conference on Management  of data, 2011, pp. 1071–1080.</li>
  <li>Tsuchiya,  Y. Sakamoto, Yuichi Tsuchimoto, and Vivian Lee, &ldquo;Big Data Processing in Cloud  Environments,&rdquo; FUJITSU Sci. Tech. J, vol. 48, no. 2, pp. 159–168, 2012</li>
  <li>Philip  Russom, &ldquo;big data analytics,&rdquo; TDWI Best Practices Report, 4 th Quarter 2011,  2011.</li>
  <li>&ldquo;Welcome  to ApacheTMHadoop®!&rdquo; [Online]. Available: http://hadoop.apache.org/. [Accessed:  25-Apr-2013].</li>
</ol>
<p><br>
</p>
</div>
        </div>
      </div>
	  <!--Start second row of columns-->
	<!--  <div class="row">
		<div class="span4">
			<p><h3>Column One</h3></span></p>
			<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed placerat sem vel nibh bibendum auctor. 
			Nullam non magna in quam egestas blandit a a justo. Integer vel rhoncus tellus. 
			Vivamus et iaculis tortor. Quisque fermentum arcu dolor. Duis mollis libero et 
			ipsum euismod sed gravida sem pretium. Aliquam eu eros at velit laoreet rhoncus. 
			Nulla a urna eu diam cursus tempor.</p>
		</div>
		<div class="span4">
			<p><h3>Column Two</h3></span></p>
			<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed placerat sem vel nibh bibendum auctor. 
			Nullam non magna in quam egestas blandit a a justo. Integer vel rhoncus tellus. 
			Vivamus et iaculis tortor. Quisque fermentum arcu dolor. Duis mollis libero et 
			ipsum euismod sed gravida sem pretium. Aliquam eu eros at velit laoreet rhoncus. 
			Nulla a urna eu diam cursus tempor.</p>
		</div>
		<div class="span4">
			<p><h3>Column Three</h3></span></p>
			<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed placerat sem vel nibh bibendum auctor. 
			Nullam non magna in quam egestas blandit a a justo. Integer vel rhoncus tellus. 
			Vivamus et iaculis tortor. Quisque fermentum arcu dolor. Duis mollis libero et 
			ipsum euismod sed gravida sem pretium. Aliquam eu eros at velit laoreet rhoncus. 
			Nulla a urna eu diam cursus tempor.</p>
		</div>
	  </div> -->

      <hr>

				<footer class="row">
			<div>
				<div class="span4">
					<div class="is-padded">
						<nav>
							<h2>Navigation</h2>
							<hr>
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="documentation.html">Documentaion</a></li>
								<li><a href="downloads.html">Downloads</a></li>
								<li><a href="contributors.html">Contributors</a></li>
							</ul>
						</nav>
					</div>
				</div>
				<div class="span4">
					<div class="is-padded">
						<h2>Follow Us</h2>
						<hr>
						<a href="https://twitter.com/percepsith">#percepsith</a>
						<p>Our officia twiiter feed. Follow us to get the latest updates..
						<br>
						</p>
						<p><br>
						  <a href="http://www.facebook.com/crowdsourcedsith">Sith-සිත්-Next-Generation-Perception-analysis</a>						</p>
						<p>This is our official facebook page. Like us.</p>
				  </div>
				</div>
				<div class="span4">
					<div class="is-padded">
				   <h2>About Us</h2>
						  <blockquote>We are trying to introduce new efficient and user friendly ways to do the perception analysis. We belive that we can take perception analysis to the next level with our effort.<br><br>
						  <em>- Team Sith</em>
						  </blockquote>
					  </div>
				</div>
			</div>
			<p>
			&copy;2013 Sith.<br>
			</p>
		</footer>

    </div><!-- /container -->

    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="js/jquery.js"></script>
    <script src="js/bootstrap-transition.js"></script>
	<script src="js/bootstrap-carousel.js"></script>
    <script src="js/bootstrap-alert.js"></script>
    <script src="js/bootstrap-modal.js"></script>
    <script src="js/bootstrap-dropdown.js"></script>
    <script src="js/bootstrap-scrollspy.js"></script>
    <script src="js/bootstrap-tab.js"></script>
    <script src="js/bootstrap-tooltip.js"></script>
    <script src="js/bootstrap-popover.js"></script>
    <script src="js/bootstrap-button.js"></script>
    <script src="js/bootstrap-collapse.js"></script>
    <script src="js/bootstrap-typeahead.js"></script>
	<script src="js/jquery-ui-1.8.18.custom.min.js"></script>
	<script src="js/jquery.smooth-scroll.min.js"></script>
	<script src="js/lightbox.js"></script>
  </body>
</html>
